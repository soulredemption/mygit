任务2 - 逻辑回归算法梳理】时长：2天
1、	逻辑回归与线性回归的联系与区别
1）线性回归要求变量服从正态分布，logistic回归对变量分布没有要求。 
2）线性回归要求因变量是连续性数值变量，而logistic回归要求因变量是分类型变量。 
3）线性回归要求自变量和因变量呈线性关系，而logistic回归不要求自变量和因变量呈线性关系 
4）logistic回归是分析因变量取某个值的概率与自变量的关系，而线性回归是直接分析因变量与自变量的关系
2、 逻辑回归的原理
逻辑回归处理的是分类问题，具体来说，是处理二分类问题。为了实现逻辑回归分类器，我们可以在线性回归的基础上（即每个特征乘以一个回归系数后相加），添加一个sigmoid函数，进而得到一个范围在0-1之间的数值。任何大于0.5的数据会被分入1类，小于0.5即被分入0类。至于为什么要用sigmoid函数，简单来说，是为了将标签归到[0,1]的范围内；深层原因，sigmoid函数的使用是由指数分布族决定的，具体内容会在下一篇博客中做详细讲解。 
3、	逻辑回归损失函数推导及优化
https://blog.csdn.net/iterate7/article/details/78992027
4、	正则化与模型评估指标
1）	正则化是为了防止过拟合，
2）	正则化的本质是为了约束和限制要优化的参数
3）	正则化可以在学习过程中降低模型复杂度和不稳定程度，从而避免过拟合的危险。
4）	L1范数：L1范数在正则化的过程中会趋向于产生少量的特征，而其他的特征都是0（L1会使得参数矩阵变得稀疏）。因此L1不仅可以起到正则化的作用，还可以起到特征选择的作用。
5）	L2范数：L2范数是通过使权重衰减，进而使得特征对于总体的影响减小而起到防止过拟合的作用的。L2的优点在于求解稳定、快速。
模型评估指标
Accuracy Precision Recall F1 score ROC曲线和AUCPR曲线
5、逻辑回归的优缺点
优点：
实现十分简单
分类时计算量非常小，训练速度快，存储资源低
可解释性强，可控性高
以概率的形输出结果，可以做排序
缺点：
容易欠拟合，分类精度不高
只能处理两分类问题，且必须线性可分
逻辑回归本身无法筛选特征。
6、	样本不均衡问题解决办法
重采样技术（Resampling）
1.1 随机欠采样（Random Under-Sampling）： 通过随机地消除占多数的类的样本来平衡类分布；直到多数类和少数类的实例实现平衡，目标才算达成。
1.2 随机过采样（Random Over-Sampling )：过采样（Over-Sampling）通过随机复制少数类来增加其中的实例数量，从而可增加样本中少数类的代表性。
1.3 基于聚类的过采样（Cluster-Based Over Sampling）：K-均值聚类算法独立地被用于少数和多数类实例。这是为了识别数据集中的聚类。随后，每一个聚类都被过采样以至于相同类的所有聚类有着同样的实例数量，且所有的类有着相同的大小。
1.4 信息性过采样：合成少数类过采样技术（SMOTE）：这一技术可用来避免过拟合——当直接复制少数类实例并将其添加到主数据集时。从少数类中把一个数据子集作为一个实例取走，接着创建相似的新合成的实例。这些合成的实例接着被添加进原来的数据集。新数据集被用作样本以训练分类模型。
算法集成技术（Algorithmic Ensemble Techniques）
上述部分涉及通过重采样原始数据提供平衡类来处理不平衡数据。集成方法的主要目的是提高单个分类器的性能。该方法从原始数据中构建几个两级分类器，然后整合它们的预测。
7. sklearn参数
https://blog.csdn.net/qq_38683692/article/details/82533460
