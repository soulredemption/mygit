一/ 机器学习的一些概念
监督学习：1.给计算机一个数据集，且告诉他正确答案（label）。我们教计算机做某件事。
		      2.主要分为回归问题（连续）和分类问题（离散）
无监督学习：1.给计算机一个数据集，没有任何提示无(label)，让计算机自己学习做某事。
2.主要为聚类算法，鸡尾酒算法。
泛化能力：计算机在练习样品的过程中总结出的模型能够适用于新的样本的能力
过拟合：1.计算机在训练样本中总结出了它认为所有样本都有的“普遍规律“。最后会导致在面对新样本的时候泛化性能下降。2.通常由于学习能力过于强大，把训练样本所包含的不太一般的性质都学到了。3.过拟合无法避免，只能缓解。
欠拟合：1.对训练样本的特征和性质尚未学好。2.通常由学习能力低下造成。3.在决策树学习中扩展分支，在神经网络学习中增加训练轮数。
交叉验证：将数据集D划分为K个互不相融的子集。每次用k-1个子集的并集作为训练集，余下子集作为测试集。可进行k次训练和测试。K一般取10，5，20。
偏差：度量了学习算法的期望预测与真实结果的偏离程度，即学习算法本身的拟合能力。
方差：度量了同样大小的训练集变动所导致的学习性能的变化。刻画了数据扰动所造成的影响。
2. 线性回归的原理
  是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。
回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。
3. 线性回归损失函数、代价函数、目标函数
代价函数：平方误差函数。在线性回归中我们需要拟合的线性模型的代价函数（误差平方和）最小，才是一个合理选择。
损失函数：定义在单个样本上的单个误差。
目标函数：最终需要优化的函数。
4. 优化方法(梯度下降法、牛顿法、拟牛顿法等)
梯度下降：用来求函数最小值的一个算法。一开始随机选一个组合，计算其代价函数，寻找下一个能让代价函数下降最多的参数组合。直到出现局部最小值。其公式于α学习率有关，决定了下降速率。α太小下降太慢， α太大，可能直接越过最低点，无法收敛。
牛顿法：用二次曲面来拟合函数的局部曲面。本质是用牛顿法求极值的应用。
拟牛顿法：该三牛顿法每次需要求解复杂的矩阵的逆矩阵的缺陷，使用正定矩阵来近似Hessian矩阵的逆，简化了运算复杂度。
5、线性回归的评估指标
R Squared，均方误差，均方根误差，评价绝对误差
6、sklearn参数详解
LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)
(1)fit_intercept:是否有截据，如果没有则直线过原点。默认为True.
说明：是否对训练数据进行中心化。如果该变量为false，则表明输入的数据已经进行了中心化，在下面的过程里不进行中心化处理；否则，对输入的训练数据进行中心化处理。
(2)normalize:是否将数据归一化。
(3)copy_X:默认为True，当为True时，X会被copied,否则X将会被覆写.。（即经过中心化，标准化后，是否把新数据覆盖到原数据上）。
(4)n_jobs:默认值为1。计算时使用的核数。

