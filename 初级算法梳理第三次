1.	信息论基础（熵 联合熵 条件熵 信息增益 基尼不纯度）
熵：表示随机变量不确定性的度量，熵越大，随机变量的不确定性就越大
联合熵：联合熵就是度量一个联合分布的随机系统的不确定度
条件熵：表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵H(Y|X)
信息增益：表示得知特征X的信息而使得类Y的信息的不确定性减少的程度（定义：特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差）
基尼不纯度：将来自集合中的某种结果随机应用于集合中某一数据项的预期误差率
2.	决策树的不同分类算法（ID3算法、C4.5、CART分类树）的原理及应用场景
ID3算法
ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。
具体方法是：从根节点（root node）开始，对结点计算所有可能的特征信息增益，选择信息增益最大的特征作为结点特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息均很小或没有特征可以选择为止。
C4.5
C4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进。C4.5在生成过程中，用信息增益比来选择特征。
CART分类树
CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。
3.	回归树原理
决策树（decision tree）也称为分类树（分类）或者回归树（数值预测）。是一种有监督的机器学习算法，是一个分类算法。在给定训练集的条件下，生成一个自顶而下的决策树，树的根为起点，树的叶子为样本的分类，从根到叶子的路径就是一个样本进行分类的过程。
决策树由结点和有向边组成。结点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类。 分类的时候，从根节点开始，对实例的某一个特征进行测试，根据测试结果，将实例分配到其子结点；此时，每一个子结点对应着该特征的一个取值。如此递归向下移动，直至达到叶结点，最后将实例分配到叶结点的类中
4.	决策树防止过拟合手段
合理、有效地抽样，用相对能够反映业务逻辑的训练集去产生决策树
剪枝：提前停止树的增长或者对已经生成的树按照一定的规则进行后剪枝。
5.	模型评估
评估指标：评估指标是把"尺子",用来评判模型优劣水平的算法,不同的机器学习模型有着不同的"尺子",同时同一种机器学习模型也可以用不同的尺子来评估，只是每个尺子的的着重点不同而已，错误率,精度,误差，精确率－召回率都是这样一把尺子
6. sklearn参数详解，Python绘制决策树
criterion:特征选择的标准，有信息增益和基尼系数两种，使用信息增益的是ID3和C4.5算法（使用信息增益比），使用基尼系数的CART算法，默认是gini系数。
splitter:特征切分点选择标准，决策树是递归地选择最优切点，spliter是用来指明在哪个集合上来递归，有“best”和“random”两种参数可以选择，best表示在所有特征上递归，适用于数据集较小的时候，random表示随机选择一部分特征进行递归，适用于数据集较大的时候。
max_depth:决策树最大深度，决策树模型先对所有数据集进行切分，再在子数据集上继续循环这个切分过程，max_depth可以理解成用来限制这个循环次数。
min_samples_split:子数据集再切分需要的最小样本量，默认是2，如果子数据样本量小于2时，则不再进行下一步切分。如果数据量较小，使用默认值就可，如果数据量较大，为降低计算量，应该把这个值增大，即限制子数据集的切分次数。
min_samples_leaf:叶节点（子数据集）最小样本数，如果子数据集中的样本数小于这个值，那么该叶节点和其兄弟节点都会被剪枝（去掉），该值默认为1。
min_weight_fraction_leaf:在叶节点处的所有输入样本权重总和的最小加权分数，如果不输入则表示所有的叶节点的权重是一致的。
max_features:特征切分时考虑的最大特征数量，默认是对所有特征进行切分，也可以传入int类型的值，表示具体的特征个数；也可以是浮点数，则表示特征个数的百分比；还可以是sqrt,表示总特征数的平方根；也可以是log2，表示总特征数的log个特征。
random_state:随机种子的设置，与LR中参数一致。
max_leaf_nodes:最大叶节点个数，即数据集切分成子数据集的最大个数。
min_impurity_decrease:切分点不纯度最小减少程度，如果某个结点的不纯度减少小于这个值，那么该切分点就会被移除。
min_impurity_split:切分点最小不纯度，用来限制数据集的继续切分（决策树的生成），如果某个节点的不纯度（可以理解为分类错误率）小于这个阈值，那么该点的数据将不再进行切分。
class_weight:权重设置，主要是用于处理不平衡样本，与LR模型中的参数一致，可以自定义类别权重，也可以直接使用balanced参数值进行不平衡样本处理。
presort:是否进行预排序，默认是False，所谓预排序就是提前对特征进行排序，我们知道，决策树分割数据集的依据是，优先按照信息增益/基尼系数大的特征来进行分割的，涉及的大小就需要比较，如果不进行预排序，则会在每次分割的时候需要重新把所有特征进行计算比较一次，如果进行了预排序以后，则每次分割的时候，只需要拿排名靠前的特征就可以了。

